# Matcha-AI-DTU Setup Guide

## Prerequisites

| Tool | Version | Purpose |
|------|---------|---------|
| Node.js | 18+ | Frontend & Orchestrator |
| Python | 3.11+ | Inference service (3.14 compatible) |
| Docker Desktop | Latest | PostgreSQL + Redis |
| FFmpeg | Latest | Video processing & highlight reel |
| NVIDIA GPU | CUDA 12.4 | GPU acceleration (optional but recommended) |

---

## Quick Start

### 1. Start Infrastructure (Docker)

```powershell
cd Matcha-AI-DTU
docker compose up -d
```

This starts:
- **PostgreSQL** on port 5433
- **Redis** on port 6380

---

### 2. Install Node Dependencies

```powershell
# Root workspace
npm install

# Frontend
cd apps/web
npm install

# Orchestrator
cd ../../services/orchestrator
npm install
npx prisma generate
npx prisma migrate deploy
```

---

### 3. Python Environment (Inference)

```powershell
cd services/inference

# Create Python virtual environment
py -3.11 -m venv venv
.\venv\Scripts\activate

# Upgrade pip
pip install --upgrade pip

# Install PyTorch with CUDA 12.4 support (if you have an NVIDIA GPU)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# Install all requirements (includes lapx, huggingface-hub, edge-tts)
pip install -r requirements.txt
```

> âœ… **`piper-tts` is no longer required.** The TTS system has been upgraded to Kokoro-82M (HuggingFace) â†’ edge-tts â†’ silent fallback. See the TTS section below.

---

### 4. Configure Environment Variables

**`services/orchestrator/.env`** (create this file):
```env
DATABASE_URL="postgresql://matcha_user:matcha_password@localhost:5433/matcha_db?schema=public"
HF_TOKEN=hf_your_huggingface_token_here
CORS_ORIGIN=http://localhost:3000
INFERENCE_URL=http://localhost:8000
PORT=4000
```

**`services/inference/.env`** (create this file):
```env
GEMINI_API_KEY=your_google_ai_studio_api_key
HF_TOKEN=hf_your_huggingface_token_here
ORCHESTRATOR_URL=http://localhost:4000
```

> ğŸ”‘ Gemini API key: [Google AI Studio](https://makersuite.google.com/app/apikey)  
> ğŸ”‘ HuggingFace token (free, "Read" scope): [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)  
> `HF_TOKEN` is optional but highly recommended for Kokoro TTS (Tier 1).

---

### 5. Start Services

**Recommended â€” single command with Turborepo:**
```powershell
# From the monorepo root
npx turbo run dev
```

**Or manually in 4 terminals:**

**Terminal 1 â€” Docker (already running)**
```powershell
docker compose up -d
```

**Terminal 2 â€” Orchestrator (port 4000)**
```powershell
cd services/orchestrator
npm run start:dev
```

**Terminal 3 â€” Inference (port 8000)**
```powershell
cd services/inference
.\venv\Scripts\activate
.\venv\Scripts\python.exe -m uvicorn main:app --host 0.0.0.0 --port 8000
```
> The `.env` file in `services/inference/` is loaded automatically by Python's `os.getenv()`.

**Terminal 4 â€” Frontend (port 3000)**
```powershell
cd apps/web
npm run dev
```

---

### 6. Access

| Service | URL |
|---------|-----|
| Frontend | http://localhost:3000 |
| Orchestrator API | http://localhost:4000 |
| Inference API | http://localhost:8000 |
| Health Check | http://localhost:8000/health |

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js   â”‚â”€â”€â”€â”€â–¶â”‚   NestJS     â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI     â”‚
â”‚   Frontend  â”‚     â”‚ Orchestrator â”‚     â”‚   Inference   â”‚
â”‚   :3000     â”‚     â”‚    :4000     â”‚     â”‚     :8000     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                         â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ PostgreSQL â”‚           â”‚   Redis    â”‚
       â”‚   :5433    â”‚           â”‚   :6380    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## TTS: 3-Tier Neural Voice System (Updated)

The TTS system has been upgraded from `piper-tts` to a 3-tier cascade for maximum quality and reliability:

| Tier | Model | Quality | Requirement |
|------|-------|---------|-------------|
| ğŸ¥‡ 1 | **Kokoro-82M** (`hexgrad/Kokoro-82M`) | Ultra-high â€” #1 TTS Arena | `HF_TOKEN` env var (free) |
| ğŸ¥ˆ 2 | **Microsoft edge-tts** (`en-GB-RyanNeural`) | High â€” neural, no key needed | `edge-tts` pip package |
| ğŸ¥‰ 3 | **FFmpeg silence** | â€” | FFmpeg in PATH |

The system tries Tier 1 â†’ falls back to Tier 2 â†’ falls back to Tier 3 automatically.

### Why we moved away from Piper TTS

| Criteria | Piper TTS (old) | Kokoro-82M (new) |
|----------|----------------|-----------------|
| **Quality** | Good | Ultra-high (#1 arena ranking) |
| **Windows install** | âœ… Pre-built wheels | âœ… via `huggingface-hub` pip |
| **API Key needed** | âŒ No | âš ï¸ Optional (free HF token) |
| **Offline** | âœ… Fully offline | âŒ API call (but edge-tts fallback is always offline) |
| **Voice style** | Generic US English | British sports commentator |

---

## Environment Variables

| Variable | Service | Required | Description |
|----------|---------|----------|-------------|
| `DATABASE_URL` | Orchestrator | âœ… | PostgreSQL connection string |
| `GEMINI_API_KEY` | Inference | âœ… | Google AI Studio key for Gemini 2.0 Flash |
| `HF_TOKEN` | Both | âš ï¸ Recommended | HuggingFace token for Kokoro-82M TTS |
| `ORCHESTRATOR_URL` | Inference | âŒ | Default: `http://localhost:4000` |
| `INFERENCE_URL` | Orchestrator | âŒ | Default: `http://localhost:8000` |
| `CORS_ORIGIN` | Orchestrator | âŒ | Default: `http://localhost:3000` |
| `PORT` | Orchestrator | âŒ | Default: `4000` |

---

## GPU Support

The inference service uses **CUDA 12.4** for GPU acceleration:

```python
import torch
print(torch.cuda.is_available())        # True if GPU detected
print(torch.cuda.get_device_name(0))    # e.g., "NVIDIA GeForce RTX 3050"
```

---

## Troubleshooting

### Port in use
```powershell
# Check what's using a port
netstat -ano | Select-String ":3000"

# Kill process by PID
Stop-Process -Id <PID> -Force
```

### Docker container conflict
```powershell
docker ps -a  # Find conflicting containers
docker stop <container_name>
```

### Python import errors
```powershell
# Activate the venv first!
.\venv\Scripts\activate

# Re-install dependencies
pip install -r requirements.txt
```

### Kokoro TTS not working
Set `HF_TOKEN` in `services/inference/.env`. Without it, the system uses anonymous HuggingFace API with strict rate limits. If Kokoro is unavailable, `edge-tts` is used automatically.

### Heatmap not appearing in Analytics tab
The heatmap is only generated if YOLO tracked players during analysis. Check inference logs for:
- `Heatmap saved â†’` (success)
- `Heatmap generation failed:` (error â€” re-analyze the match)

### Prisma `generate` fails with EPERM
The orchestrator process has the Prisma DLL locked. Stop the service first, then run `npx prisma generate`, then restart.

---

## File Structure

```
Matcha-AI-DTU/
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ web/               # Next.js frontend
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ orchestrator/      # NestJS API + WebSocket
â”‚   â”‚   â””â”€â”€ prisma/        # Schema + migrations
â”‚   â””â”€â”€ inference/         # FastAPI + YOLO + Kokoro TTS
â”‚       â”œâ”€â”€ app/core/      # analysis.py, heatmap.py, goal_detection.py
â”‚       â”œâ”€â”€ venv/          # Python virtualenv (gitignored)
â”‚       â”œâ”€â”€ yolov8s.pt     # YOLOv8 small model weights
â”‚       â””â”€â”€ uploads/       # Uploaded videos + generated assets
â”œâ”€â”€ uploads/               # Shared upload directory
â””â”€â”€ docker-compose.yml     # PostgreSQL + Redis
```
