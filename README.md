# Matcha-AI-DTU

Matcha-AI-DTU is a comprehensive AI-powered sports video analysis platform. It processes sports footage (specifically soccer/football), tracks the ball, automatically detects goals and events, generates insightful commentary using LLMs (Gemini), and synthesizes audio voiceovers for highlight reels.

This is a **monorepo** built with Next.js, NestJS, and FastAPI, orchestrated with Docker.

---

## ğŸ— Architecture Overview

The system is broken down into three main applications:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Next.js   â”‚â”€â”€â”€â”€â–¶â”‚   NestJS     â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI     â”‚
â”‚   Frontend  â”‚     â”‚ Orchestrator â”‚     â”‚   Inference   â”‚
â”‚   :3000     â”‚     â”‚    :4000     â”‚     â”‚     :8000     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                         â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ PostgreSQL â”‚           â”‚   Redis    â”‚
       â”‚   :5433    â”‚           â”‚   :6380    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **[Frontend (apps/web)](./apps/web/README.md)**: A modern React interface built with Next.js 15 and Tailwind CSS. Provides video upload capabilities, live processing status via WebSockets, and a feature-rich dashboard for viewing analyzed clips and highlights.
2. **[Orchestrator (services/orchestrator)](./services/orchestrator/README.md)**: A robust NestJS API backend. It handles database operations (Prisma + PostgreSQL), state management (Redis), WebSocket communication with the frontend, and delegates heavy processing tasks to the Inference service.
3. **[Inference Engine (services/inference)](./services/inference/README.md)**: The core AI powerhouse built on FastAPI. It runs advanced Computer Vision models (YOLOv8) for ball tracking and goal detection, integrates with Google's Gemini API for dynamic contextual commentary, and utilizes Piper TTS for generating realistic voiceovers.

---

## âœ¨ Key Features

- **Automated Video Analysis**: Upload raw sports footage and let the system automatically analyze the content.
- **Advanced Goal Detection**: Custom Goal Detection Engine built with YOLO that auto-calibrates to the goal line, tracks the ball across frames, and confirms goals with high precision.
- **Action Event Recognition**: Detects other gameplay events (fouls, tackles, celebrations) augmenting the analysis.
- **AI Commentary Generation**: Context-aware, natural-sounding commentary generated by the Gemini API describing the on-field action.
- **Text-To-Speech (TTS) Pipeline**: Uses Piper TTS for seamless, offline, high-quality voiceover generation.
- **Real-Time Progress Tracking**: WebSocket integration provides users with live updates across the 4-phase analysis pipeline.

---

## ğŸš€ Quick Start Guide

### Prerequisites

| Tool | Version | Purpose |
|------|---------|---------|
| Node.js | 18+ | Frontend & Orchestrator |
| Python | 3.11/3.12 | Inference service |
| Docker Desktop | Latest | Required for PostgreSQL & Redis containers |
| FFmpeg | Latest | Required for Video processing |
| NVIDIA GPU | CUDA 12.4 | Recommended for fast PyTorch acceleration |

### 1. Launch Infrastructure
Start the database and Redis cache utilizing Docker Compose:
```bash
docker compose up -d
```
*This starts PostgreSQL on port 5433 and Redis on port 6380.*

### 2. Install Node Dependencies
Install all JavaScript dependencies seamlessly via turbo:
```bash
npm install
```
Then, deploy the database schema in the orchestrator:
```bash
cd services/orchestrator
npx prisma generate
npx prisma migrate deploy
```

### 3. Setup Python Inference Environment
The inference service requires its own Python environment with specific dependencies:
```bash
cd services/inference

# Create & activate a Python 3.11 virtual environment
py -3.11 -m venv venv
.\venv\Scripts\activate

# Upgrade pip
pip install --upgrade pip

# Install PyTorch with CUDA 12.4 support (if you have an NVIDIA GPU)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# Install the rest of the dependencies
pip install -r requirements.txt
pip install piper-tts
```

### 4. Configure Environment Variables
Create `.env` files where required.
For **services/inference**, set up the following environment variables (or export them):
- `ORCHESTRATOR_URL="http://localhost:4000"`
- `GEMINI_API_KEY="your-gemini-api-key"`

### 5. Start the Services

You will need multiple terminal windows (or multiplexer panes) to run all services simultaneously.

**Terminal 1 - Orchestrator (Port 4000)**
```bash
cd services/orchestrator
npm run start:dev
```

**Terminal 2 - Inference Engine (Port 8000)**
*Ensure the python virtual environment is activated*
```bash
cd services/inference
# (if Windows Powershell)
$env:ORCHESTRATOR_URL="http://localhost:4000"
$env:GEMINI_API_KEY="your-api-key"
uvicorn main:app --host 0.0.0.0 --port 8000
```

**Terminal 3 - Frontend Web App (Port 3000)**
```bash
cd apps/web
npm run dev
```

---

## ğŸ›  Project Structure

```text
Matcha-AI-DTU/
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ web/              # Next.js 15 Frontend
â”œâ”€â”€ packages/             # Shared local packages (if any)
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ orchestrator/     # NestJS API, WebSockets, Prisma DB access
â”‚   â””â”€â”€ inference/        # Python FastAPI, YOLOv8 CV Models, TTS, Gemini LLM
â”œâ”€â”€ uploads/              # Shared video/audio uploads directory
â”œâ”€â”€ SETUP.md              # Detailed local setup commands
â”œâ”€â”€ GOAL_DETECTION_INTEGRATION.md # Docs on the Goal Detection feature
â””â”€â”€ docker-compose.yml    # Defines Postgres + Redis setup
```

---

## ğŸ’¡ Troubleshooting Solutions

- **Ports already in use:** Use `netstat -ano | Select-String ":3000"` to find and kill conflicting operations.
- **Postgres/Redis connection failures:** Ensure Docker daemon is running and containers haven't exited (`docker ps -a`).
- **Python Import errors (TTS/Torch):** Double-check that your `venv` is activated. Piper TTS should be installed separately via `pip install piper-tts` which provides pre-built binaries without requiring heavy C++ compilation.
- **Analysis stuck at 0%:** Ensure `ORCHESTRATOR_URL` is correct in the Inference service. The WebSocket link may be unreachable.

---
*Built for Matcha-AI-DTU*
