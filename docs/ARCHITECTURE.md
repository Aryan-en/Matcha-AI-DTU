# ğŸ— System Architecture

Matcha-AI-DTU utilizes a microservice-like **Monorepo** architecture leveraging specific languages for their native strengths: 
- **TypeScript/React** for dynamic, real-time UI mapping.
- **TypeScript/NestJS** for strictly-typed API gateways and WebSockets.
- **Python/FastAPI** for deep learning AI inference processing.

This document serves to visually explain how data flows across the monorepo when a user initiates a request.

---

## ğŸŒŠ The Core Video Analysis Data flow 

The primary end-to-end operation is ingesting a raw video, processing it through YOLO Computer Vision AI models and Large Language Models, and returning a generated Sports Highlight audio synthetic file dynamically to the browser. 

```mermaid
sequenceDiagram
    autonumber
    
    actor User as Client (Next.js App)
    participant Orch as Orchestrator (NestJS)
    participant Redis as Redis Cache
    participant DB as PostgreSQL (Prisma)
    participant ML as Inference Engine (Python)

    %% -------------------------------------
    %% Stage 1: Upload & Registration Focus
    %% -------------------------------------
    User->>Orch: POST /matches (Uploads Video File)
    activate Orch
    
    Orch->>DB: Create Match Record & Return ID
    Orch->>Redis: Set Processing State [Analysis Queue]
    
    Orch->>ML: POST /analyze { videoPath, matchId }
    activate ML
    
    Orch-->>User: HTTP 201 Created { matchId }
    deactivate Orch
    
    %% -------------------------------------
    %% Stage 2: Processing & Live Updates
    %% -------------------------------------
    
    note over ML,User: Real-Time Event Loop via WebSockets
    
    loop Per Video Frame Batch
        ML->>ML: Run Phase 1b: Goal Detection Pipeline
        ML->>ML: Run Phase 2: Action parsing 
        
        ML->>Orch: POST /callbacks/progress { matchId, progress: x% }
        Orch->>User: WS Event "analysisProgress" [Progress Bar Update]
        
        opt Event Detected (e.g. GOAL!)
            ML->>Orch: POST /callbacks/event { ...EventData }
            Orch->>DB: Save Event Record
            Orch->>User: WS Event "eventDetected" [Live UI Popup]
        end
    end
    
    %% -------------------------------------
    %% Stage 3: Generative AI & Audio Render
    %% -------------------------------------
    
    ML->>ML: Formulate script using Google Gemini
    ML->>ML: Render voiceover via Piper TTS (.wav)
    
    ML->>Orch: POST /callbacks/complete { highlightMetadata }
    deactivate ML
    activate Orch
    
    Orch->>DB: Update Match Status = "COMPLETED"
    Orch->>Redis: Clear Cache Queue
    
    Orch-->>User: WS Event "analysisComplete" [Refresh App Data]
    deactivate Orch
    
    User->>Orch: GET /matches/{id}
    Orch-->>User: Return full payload with AI Audio URL
```

---

## ğŸ—„ï¸ Database Schema Diagram

We utilize Prisma ORM for type-safe database queries. The central entities revolve around `Match` (a video entity) and its children `MatchEvent`.

```mermaid
erDiagram
    MATCH {
        String id PK
        String title
        String videoUrl
        String ttsAudioUrl
        String status
        DateTime createdAt
        DateTime updatedAt
    }

    MATCH_EVENT {
        String id PK
        String matchId FK
        String eventType "GOAL, TACKLE, FOUL, etc."
        Float timestampSeconds
        Float confidence
        String description
    }
    
    MATCH ||--o{ MATCH_EVENT : "has many"
```

---

## ğŸ”Œ WebSockets Implementation (Socket.io)

For real-time progression we decouple the heavy Python operations from holding HTTP connections open using Callbacks and WebSockets.

1. **NestJS** mounts a Socket.IO Gateway on port `4000`.
2. When the **Inference (Python)** script processes frames, it issues a synchronous *fire-and-forget* HTTP request to the Orchestrator (`/callbacks/progress`).
3. The **Orchestrator** translates this HTTP payload into an active Socket Event mapped to all connected Next.js users listening to that namespace.
4. If a WebSocket disconnects, the analysis **continues uninterrupted** inside the Python environment, preventing dropped progress upon a user refreshing their browser.

---

## ğŸ†• Updated Database Schema (v2 â€” Full Analytics Platform)

Since the initial architecture, the Prisma schema has been significantly expanded to support the full analytics platform. Migration `20260221180240_add_heatmap_speed` was the most recent, adding heatmap and speed fields.

The complete current schema is documented below:

```mermaid
erDiagram
    MATCH {
        String   id              PK
        String   uploadUrl
        String   status          "UPLOADED | PROCESSING | COMPLETED | FAILED"
        Int      progress        "0â€“100 processing percent"
        Float    duration        "total video length in seconds"
        String   summary         "AI Gemini 3-5 sentence narrative"
        String   highlightReelUrl "URL to stitched MP4 highlight reel"
        Json     trackingData    "YOLO bbox frames: [{t, b[], p[]}]"
        Json     teamColors      "[[R,G,B],[R,G,B]] KMeans jersey clusters"
        String   heatmapUrl      "URL to player density heatmap PNG (NEW)"
        Float    topSpeedKmh     "95th-pct peak ball speed km/h (NEW)"
        DateTime createdAt
        DateTime updatedAt
    }

    EVENT {
        String   id         PK
        String   matchId    FK
        Float    timestamp  "seconds, 0.01s precision"
        String   type       "GOAL | FOUL | TACKLE | SAVE | CELEBRATION | HIGHLIGHT | PENALTY | RED_CARD | YELLOW_CARD | CORNER | OFFSIDE"
        Float    confidence "0.0â€“1.0 detection confidence"
        Float    finalScore "0â€“10 composite weighted context score"
        String   commentary "Gemini-generated broadcast commentary text"
        DateTime createdAt
    }

    EMOTION_SCORE {
        String   id            PK
        String   matchId       FK
        Float    timestamp
        Float    audioScore    "0.0â€“1.0 motion-derived intensity"
        Float    motionScore   "0.0â€“1.0 OpenCV frame-diff score"
        Float    contextWeight "0.0â€“1.0 time-position temporal weight"
        Float    finalScore    "0â€“10 composite"
        DateTime createdAt
    }

    HIGHLIGHT {
        String   id         PK
        String   matchId    FK
        Float    startTime  "clip start in seconds"
        Float    endTime    "clip end in seconds"
        Float    score      "0â€“10 highlight significance"
        String   eventType  "GOAL | SAVE | TACKLE etc."
        String   commentary "highlight description"
        String   videoUrl   "optional clip URL"
        DateTime createdAt
    }

    MATCH  ||--o{ EVENT         : "has many"
    MATCH  ||--o{ EMOTION_SCORE : "has many"
    MATCH  ||--o{ HIGHLIGHT     : "has many"
```

---

## ğŸ“Š Analytics Pipeline â€” Phase 5 (NEW)

After the main event detection loop completes, a **Phase 5 Post-Processing Analytics** pass runs entirely within the inference service using the `track_frames` data accumulated by YOLO during the primary loop. No additional video re-read is needed.

```mermaid
graph TD
    A["YOLO Frame Loop\n(Phases 1â€“4 complete)"] --> TF["track_frames list\ncollected during the loop"]
    TF --> H["generate_heatmap()\napp/core/heatmap.py"]
    TF --> S["estimate_ball_speed()\napp/core/heatmap.py"]
    TF --> TC["_cluster_teams()\napp/core/analysis.py\nK-Means jersey clustering"]
    
    H --> HI["heatmap_matchId.png\nOpenCV top-down pitch render\nGaussian-blurred density overlay\nsaved â†’ /uploads/"]
    S --> SP["topSpeedKmh float\n95th-percentile ball speed\n0â€“200 km/h clamped"]
    TC --> CO["teamColors [[R,G,B],[R,G,B]]\nused for heatmap colour coding\nand frontend swatch display"]

    HI --> PL["Orchestrator PATCH\nPOST /matches/:id/complete"]
    SP --> PL
    CO --> PL
    PL --> DB["PostgreSQL\nheatmapUrl  topSpeedKmh  teamColors"]
    DB --> FE["Frontend Analytics Tab\nâš¡ Ball Speed Â· ğŸ¨ Team Colors Â· ğŸ—º Heatmap"]
```

### Phase 5 Implementation Details

| Component | File | Algorithm |
|---|---|---|
| Heatmap generator | `app/core/heatmap.py` | For each tracked player, accumulate centroid `+1` into a 2D NumPy grid. Run `cv2.GaussianBlur(kernel=51)`, then overlay blended colour per team onto a pitch background drawn with OpenCV primitives |
| Ball speed estimator | `app/core/heatmap.py` | Extract consecutive ball `(cx_norm, cy_norm)` positions. Multiply by real pitch dims (105m Ã— 68m) to get metres. Divide by `Î”t â†’ m/s â†’ km/h`. Return 95th-percentile to suppress tracking noise spikes |
| Team colour detector | `app/core/analysis.py` | `_crop_jersey()` extracts the torso region (30â€“70% height, 20â€“80% width) of each detected player box. `_dominant_colour()` computes the median RGB. `_cluster_teams()` runs NumPy K-Means (20 iterations, 2 centroids) to split players into two jersey-colour groups |
| Analytics tab (frontend) | `apps/web/app/matches/[id]/page.tsx` | New `"analytics"` tab state. Renders: speed with amber glow text, team hex swatches with box-shadow glow, `<img>` heatmap tag loaded from `match.heatmapUrl` |

---

## ğŸ—£ï¸ TTS (Text-to-Speech) Architecture â€” 3-Tier System (UPDATED)

The commentary voiceover system has been upgraded from a single `edge-tts` implementation to a **3-tier cascading fallback chain** ensuring the highest quality audio is always used while guaranteeing production is never blocked.

```mermaid
graph TD
    T["tts_generate(text, output_path)\ncalled once per highlight clip"] --> K

    K["ğŸ¥‡ Tier 1: Kokoro-82M\nhexgrad/Kokoro-82M\nHuggingFace Inference API\nVoice: af_sky (British female)\n#1 ranked â€” TTS Arena Leaderboard"]
    K -- "âœ… HTTP 200 + audio bytes" --> OUT
    K -- "âŒ Rate-limited / ImportError / timeout" --> E

    E["ğŸ¥ˆ Tier 2: Microsoft edge-tts\nen-GB-RyanNeural\nBritish male broadcaster\nNo API key required â€” always available"]
    E -- "âœ… asyncio.run + .save()" --> OUT
    E -- "âŒ Network error / not installed" --> S

    S["ğŸ¥‰ Tier 3: FFmpeg anullsrc\nSilent 44100Hz stereo audio\nAbsolute fallback â€” never fails"]
    S --> OUT

    OUT["Audio file written to disk\nMixed into highlight reel via FFmpeg"]
```

**Configuration constants in `analysis.py`:**
```python
_KOKORO_MODEL   = "hexgrad/Kokoro-82M"   # Top TTS Arena model
_KOKORO_VOICE   = "af_sky"               # British female sports commentator
_EDGE_TTS_VOICE = "en-GB-RyanNeural"     # British male broadcaster (fallback)
_hf_token       = os.getenv("HF_TOKEN")  # Set in services/inference/.env
```

---

## ğŸ–¥ï¸ Frontend Component Architecture

```
apps/web/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ page.tsx                   Hero landing page
â”‚   â”‚                              Full-screen video bg + overlay stats
â”‚   â”œâ”€â”€ layout.tsx                 Root layout â€” Navbar + Footer injected globally
â”‚   â”œâ”€â”€ globals.css                CSS design tokens, animations
â”‚   â”‚                              .hide-scrollbar utility class (NEW)
â”‚   â””â”€â”€ matches/
â”‚       â”œâ”€â”€ page.tsx               Match Dashboard
â”‚       â”‚                          Status filters: UPLOADED / PROCESSING / COMPLETED / FAILED
â”‚       â”‚                          Responsive card grid (Android-optimised)
â”‚       â””â”€â”€ [id]/
â”‚           â””â”€â”€ page.tsx           Match Detail Page
â”‚               â”œâ”€â”€ Video Player   Seeks to YOLO-detected event timestamps on click
â”‚               â”œâ”€â”€ Top 5 Moments  Horizontally scrollable podium cards (gold/silver/bronze)
â”‚               â”œâ”€â”€ Intensity Chart Motion score sparkline over match duration
â”‚               â”œâ”€â”€ Highlights Tab Highlight clip cards with TTS commentary
â”‚               â”œâ”€â”€ Events Tab     Full event timeline + live WebSocket feed during processing
â”‚               â””â”€â”€ Analytics Tab  â† NEW
â”‚                   â”œâ”€â”€ âš¡ Ball Speed   95th-pct peak km/h from YOLO tracking
â”‚                   â”œâ”€â”€ ğŸ¨ Team Colors  Auto-detected jersey swatches (hex codes)  
â”‚                   â””â”€â”€ ğŸ—º Heatmap     OpenCV player density PNG with pitch overlay
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ layout/
â”‚   â”‚   â”œâ”€â”€ Navbar.tsx             Responsive top nav with mobile hamburger
â”‚   â”‚   â””â”€â”€ Footer.tsx             Global footer
â”‚   â”œâ”€â”€ match-dashboard.tsx        Filterable card grid
â”‚   â”‚                              Responsive filter tabs + stat pills (Android-optimised)
â”‚   â””â”€â”€ ui/                        shadcn/ui component primitives
```

---

## ğŸ” Environment Variables Reference

All services load `.env` from their own service directory. Never commit these files.

### `services/orchestrator/.env`

| Variable | Required | Default | Description |
|---|---|---|---|
| `DATABASE_URL` | âœ… Yes | â€” | PostgreSQL connection string (`postgresql://user:pass@host:port/db`) |
| `HF_TOKEN` | âš ï¸ Recommended | anonymous | HuggingFace token â€” raises Kokoro TTS rate limits significantly |
| `PORT` | âŒ | `4000` | NestJS HTTP + WebSocket port |
| `CORS_ORIGIN` | âŒ | `http://localhost:3000` | Comma-separated list of allowed frontend origins |
| `INFERENCE_URL` | âŒ | `http://localhost:8000` | Base URL to the Python inference service |
| `REQUEST_TIMEOUT` | âŒ | `30000` | HTTP request timeout in milliseconds |

### `services/inference/.env`

| Variable | Required | Default | Description |
|---|---|---|---|
| `GEMINI_API_KEY` | âœ… Yes | â€” | Google AI Studio API key for Gemini 2.0 Flash (commentary + summary) |
| `HF_TOKEN` | âš ï¸ Recommended | anonymous | HuggingFace token for Kokoro-82M TTS (Tier 1) |
| `ORCHESTRATOR_URL` | âŒ | `http://localhost:4000` | Orchestrator base URL for progress callbacks |

---

## ğŸ“¦ Complete Monorepo Directory Layout

```
Matcha-AI-DTU/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ web/                        Next.js 14 frontend (App Router)
â”‚   â”‚   â”œâ”€â”€ app/                    Pages and layouts
â”‚   â”‚   â”œâ”€â”€ components/             Shared React components
â”‚   â”‚   â””â”€â”€ public/                 Static assets
â”‚   â””â”€â”€ mobile/                     Expo React Native (future)
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ orchestrator/               NestJS API gateway (port 4000)
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ matches/            Match upload, analysis, retrieval module
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ matches.controller.ts
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ matches.service.ts
â”‚   â”‚   â”‚   â””â”€â”€ events/             Socket.IO real-time gateway
â”‚   â”‚   â”‚       â””â”€â”€ events.gateway.ts
â”‚   â”‚   â””â”€â”€ prisma/
â”‚   â”‚       â”œâ”€â”€ schema.prisma       Full Prisma ORM schema
â”‚   â”‚       â””â”€â”€ migrations/         All applied DB migrations
â”‚   â”‚
â”‚   â””â”€â”€ inference/                  Python FastAPI AI engine (port 8000)
â”‚       â”œâ”€â”€ app/
â”‚       â”‚   â”œâ”€â”€ core/
â”‚       â”‚   â”‚   â”œâ”€â”€ analysis.py     Main 5-phase pipeline orchestrator
â”‚       â”‚   â”‚   â”œâ”€â”€ heatmap.py      Player heatmap + ball speed (NEW)
â”‚       â”‚   â”‚   â”œâ”€â”€ goal_detection.py  Kalman + Homography + FSM goal engine
â”‚       â”‚   â”‚   â””â”€â”€ soccernet_detector.py  SoccerNet trained event detector
â”‚       â”‚   â””â”€â”€ api/
â”‚       â”‚       â””â”€â”€ routes.py       FastAPI route definitions
â”‚       â”œâ”€â”€ yolov8n.pt              YOLOv8 nano model weights
â”‚       â”œâ”€â”€ yolov8s.pt              YOLOv8 small model weights (default)
â”‚       â”œâ”€â”€ requirements.txt        Python dependencies
â”‚       â””â”€â”€ AI_PIPELINE.md          Detailed pipeline documentation
â”‚
â”œâ”€â”€ uploads/                        Shared video + generated asset directory
â”‚                                   (match videos, heatmap PNGs, highlight reels)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md             â† This file
â”‚   â”œâ”€â”€ API_REFERENCE.md            HTTP + WebSocket API contracts
â”‚   â””â”€â”€ CONTRIBUTING.md             Contribution guidelines
â”‚
â”œâ”€â”€ turbo.json                      Turborepo task pipeline config
â””â”€â”€ package.json                    Monorepo root workspace config
```
